{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c56406",
   "metadata": {},
   "source": [
    "### What is Deep Learning?\n",
    "Deep Learning is a subfield of Artificial Intelligence and Machine Learning that is inspired by the structure of the human brain. \n",
    "\n",
    "![Machine Learning vs Deep Learning](https://d2u1z1lopyfwlx.cloudfront.net/thumbnails/43ab59d8-6509-5ca1-b6a0-2121062c0c89/c9236736-5ec3-548f-9f17-4657d0612d0f.jpg)\n",
    "\n",
    "Where machine learning is learning from some input data and gives a <ins>statistical output</ins>.  \n",
    "And Deep Learning depends on a <u>logical structure</u> called Neural Network inspired by the human brain. Here, many perceptrons (circles) are connected with one another using weights (arrow symbols). Each column of perceptrons is called a layer. There are 3 types of layers:  \n",
    "1. Input Layer  \n",
    "2. Output Layer  \n",
    "3. Hidden Layers (There can be many hidden layers.)  \n",
    "\n",
    "Some neural networks are: ANN, CNN (for images), RNN (for speech/text), GAN (for generating text, images, etc.), etc. \n",
    "\n",
    "### Why is Deep Learning Getting So Famous?\n",
    "\n",
    "- **Applicability**  \n",
    "  Deep learning is useful in many areas like images, speech, and text, making it very versatile.\n",
    "\n",
    "- **Performance**  \n",
    "  It delivers high accuracy and often beats traditional methods, especially in complex tasks involving large data.\n",
    "\n",
    "This wide usability and superior results explain its rising popularity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b99ad8",
   "metadata": {},
   "source": [
    "### History of Deep Learning with AI Winter\n",
    "\n",
    "- **1943:** Warren McCulloch and Walter Pitts created the first computational model of neural networks, laying the foundation for artificial intelligence.\n",
    "\n",
    "- **1958:** Frank Rosenblatt developed the **Perceptron**, an early neural network algorithm for pattern recognition.\n",
    "\n",
    "- **1969 - AI Winter:** Marvin Minsky and Seymour Papert published *Perceptrons*, which revealed critical limitations of single-layer neural networks, such as their inability to solve the XOR problem. This critique led to widespread skepticism and a significant reduction in funding and research enthusiasm for neural networks, marking the start of the first AI winter—a period of slowdown and disillusionment for AI research lasting into the 1970s and early 1980s.\n",
    "\n",
    "- In **1974**, **Paul Werbos** proposed using backpropagation specifically to train neural networks during his PhD thesis. He linked the reversibility of error flow with psychological theories and suggested gradient-based training for multilayer networks.\n",
    "\n",
    "- **1980s:** In the early 1980s, **David E. Rumelhart**, **Geoffrey Hinton**, and **Ronald J. Williams** independently popularized the use of backpropagation for multi-layer networks through their influential 1986 paper. Their work is often credited for making backpropagation widely known and used in training neural networks. \n",
    "The resurgence of AI research with expert systems and the popularization of **backpropagation** for training multi-layer networks revitalized neural network research despite continuing challenges.\n",
    "\n",
    "- **Late 1980s to Early 1990s - Second AI Winter:** Following initial enthusiasm with expert systems, interest and funding sharply declined again due to limitations in AI scaling, reduced defense funding, and failed commercial promises, causing many AI companies to fail and skepticism to grow.\n",
    "\n",
    "- **1990s:** Introduction of **Long Short-Term Memory (LSTM)** networks helped overcome issues in training recurrent networks on long sequences.\n",
    "\n",
    "- **2006:** Geoffrey Hinton and colleagues popularized **Deep Belief Networks (DBNs)** using unsupervised pre-training for feedforward networks, further reviving deep learning.\n",
    "\n",
    "- **2010s-Present:** Advances accelerated due to large datasets, powerful GPUs, improved algorithms, and popular frameworks. Landmark achievements include AlexNet's success in 2012 and breakthroughs in architectures like ResNet and Transformers, leading to wide adoption in industry and research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19899d8",
   "metadata": {},
   "source": [
    "### Broader Understanding of Deep Learning:\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial neural networks with <u>representation learning</u>.\n",
    "\n",
    "Deep learning algorithms use multiple layers to progressively extract higher-level features from raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify concepts relevant to humans such as digits, letters, or faces.\n",
    "\n",
    "\n",
    "### Feature Learning (Representation Learning)\n",
    "<u>Feature learning</u>, also called <u>representation learning</u>, is the process where a machine learning model automatically discovers the important features or representations from raw data without manual intervention. Deep learning models with neural networks excel at this by learning hierarchical features through their layers.\n",
    "\n",
    "### Feature Engineering\n",
    "<u>Feature engineering</u> is the manual process of selecting, designing, and transforming raw data into features based on domain knowledge to improve model performance.\n",
    "\n",
    "\n",
    "### Key Difference:\n",
    "- **Feature Learning** is automatic and discovers features from data during training.  \n",
    "- **Feature Engineering** is manual and requires human expertise to create useful features before training.\n",
    "\n",
    "Deep learning's ability to perform feature learning eliminates much of the manual work of feature engineering, making it especially powerful with large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b602a4b6",
   "metadata": {},
   "source": [
    "### Deep Learning vs Machine Learning:\n",
    "\n",
    "1. **Data Dependency:** Deep learning requires much more data. Machine learning works better with smaller datasets, but as data increases, deep learning delivers outstanding results.\n",
    "\n",
    "### Learning Performance: Machine Learning vs Deep Learning\n",
    "\n",
    "This graph shows how Machine Learning and Deep Learning perform as the amount of data increases. Machine Learning starts with better performance on small data but plateaus as data grows. Deep Learning starts lower but surpasses Machine Learning and continues to improve significantly with more data.\n",
    "\n",
    "<img src=\"https://ppl-ai-code-interpreter-files.s3.amazonaws.com/web/direct-files/629db7d530be5194bd061045c2c45563/f907c6de-a9b0-42ca-97f4-90805cc25078/79b35466.png\" alt=\"Learning Performance Graph\" width=\"500\" />\n",
    "\n",
    "\n",
    "2. **Hardware Dependency:** Deep learning needs powerful GPUs with large memory. Machine learning benefits from a decent multi-core CPU (e.g., Intel i7/i9 or AMD Ryzen 7/9) for data preprocessing and other tasks, but does not always require high-end CPUs.\n",
    "\n",
    "3. **Training Time:** Deep learning takes longer to train but requires less time for prediction.\n",
    "\n",
    "4. **Feature Selection:** Deep learning uses <u>feature learning</u>, which is automatic and discovers features during training. Therefore, no domain experts are needed for feature selection.\n",
    "\n",
    "5. **Interpretability:** We cannot easily interpret the hidden layers in deep learning, so it is difficult to explain why a certain output is produced.\n",
    "\n",
    "6. **Complexity:** Deep learning models are more complex, with multiple layers mimicking human brain neurons, while machine learning models are generally simpler.\n",
    "\n",
    "7. **Use Cases:** Machine learning is suitable for structured data and simpler problems. Deep learning excels with unstructured data like images, audio, and text, solving more complex problems.\n",
    "\n",
    "\n",
    "\n",
    "### Why Machine Learning Cannot Be Fully Replaced by Deep Learning\n",
    "\n",
    "Despite deep learning’s strengths, machine learning remains essential for the following reasons:\n",
    "\n",
    "- It works better with **small to medium datasets**, where deep learning may underperform.\n",
    "- It requires less **computational power** and resources.\n",
    "- It offers better **interpretability** of models.\n",
    "- Suitable for many problems with structured data where manual **feature engineering** improves results.\n",
    "- Faster to develop and train for simpler models and applications.\n",
    "\n",
    "Thus, both have important roles depending on the specific task and constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca520a5",
   "metadata": {},
   "source": [
    "### How Deep Learning Grew So Much Now?\n",
    "\n",
    "1. **Labeled Datasets:**  \n",
    "Since around 2012, data generation has grown exponentially. The availability of large public datasets like ImageNet and COCO made research much easier by providing the vast amounts of data needed to train deep learning models effectively.\n",
    "\n",
    "2. **Frameworks:**  \n",
    "The development of easy-to-use deep learning frameworks such as TensorFlow, PyTorch, and Keras made designing, training, and deploying neural networks accessible to researchers and developers worldwide.\n",
    "\n",
    "3. **Architecture:**  \n",
    "Advances in neural network architectures like CNNs, RNNs, LSTMs, GANs, and Transformers introduced powerful models for different data types and tasks, greatly improving performance.  \n",
    "<u>**Transfer learning**</u> is a key technique where a model trained on one task is reused and fine-tuned for a related task, enabling faster training and better results with less data.  \n",
    "Popular models in the community include:  \n",
    "- **ResNet:** Deep residual networks for image recognition  \n",
    "- **BERT:** Pre-trained transformers for natural language processing  \n",
    "- **UNet:** Medical image segmentation  \n",
    "- **Pix2Pix:** Image-to-image translation  \n",
    "- **YOLO:** Real-time object detection  \n",
    "- **WaveNet:** Audio generation  \n",
    "\n",
    "4. **Hardware:**  \n",
    "According to **Moore's Law**, computing power doubles approximately every two years while the cost of technology decreases, making advanced hardware more accessible.  \n",
    "GPUs dramatically reduce training time for deep models. Additionally, custom hardware is designed to optimize deep learning:  \n",
    "- **FPGA (Field Programmable Gate Arrays):** Reconfigurable chips offering energy-efficient and flexible deep learning acceleration.  \n",
    "- **ASICs (Application-Specific Integrated Circuits):** Specialized hardware designed for specific deep learning tasks, including:  \n",
    "  - **TPUs (Tensor Processing Units):** Google's AI-specific chips for accelerating neural network training and inference.  \n",
    "  - **Edge TPUs:** Optimized for low-power devices at the edge.  \n",
    "  - **NPUs (Neural Processing Units):** Dedicated processors for AI workloads in mobile and embedded systems.\n",
    "\n",
    "5. **Community:**  \n",
    "A vibrant open-source community consistently shares research, code, tutorials, and pre-trained models, accelerating adoption and innovation.  \n",
    "Many influential architectures and models like ResNet, BERT, UNet, Pix2Pix, YOLO, and WaveNet demonstrate collaborative progress and practical success across different AI fields.  \n",
    "\n",
    "- **ResNet (Residual Network):**  \n",
    "A deep learning architecture designed to make training very deep neural networks easier and more effective. It uses **skip connections** (or shortcuts) that allow the network to learn residual functions. This helps tackle the vanishing gradient problem and enables very deep models to perform well.\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers):**  \n",
    "A transformer-based model primarily used for natural language processing tasks. BERT learns context from both directions (left and right) in a sentence, making it highly effective for tasks like question answering, sentiment analysis, and text classification.\n",
    "\n",
    "- **UNet:**  \n",
    "A convolutional neural network architecture used for image segmentation, especially in medical imaging. It has a U-shaped structure with encoder and decoder paths that capture context and precise localization.\n",
    "\n",
    "- **Pix2Pix:**  \n",
    "A generative adversarial network (GAN) model used for image-to-image translation tasks, like converting sketches to photos or day images to night images.\n",
    "\n",
    "- **YOLO (You Only Look Once):**  \n",
    "A real-time object detection system known for its speed and accuracy. YOLO predicts bounding boxes and class probabilities directly from full images in a single evaluation.\n",
    "\n",
    "- **WaveNet:**  \n",
    "A deep generative model of raw audio waveforms used for text-to-speech synthesis and other audio generation tasks. It produces highly realistic and natural-sounding audio.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5e28ad",
   "metadata": {},
   "source": [
    "### What is a Loss Function?\n",
    "A **loss function** is a method of evaluating how well your algorithm is modeling your dataset.\n",
    "\n",
    "- If the loss value is **high**, it means the model is performing poorly.  \n",
    "- If the loss value is **low**, it means the model is performing well.  \n",
    "- It is a mathematical function that takes the modelâ€™s parameters as input.  \n",
    "- To improve the model, we adjust its parameters to minimize the loss.\n",
    "\n",
    "\n",
    "### Why is the Loss Function Important?\n",
    "> *\"You can't improve what you can't measure.\"*\n",
    "\n",
    "- In machine learning, the loss function is like the **eye of the model**.  \n",
    "  It guides the model on what to do next and what to improve.  \n",
    "- In deep learning, we improve the weights and biases using **gradient descent**, which relies on the loss function.\n",
    "\n",
    "\n",
    "### Loss Functions in Deep Learning\n",
    "Different problems in deep learning require different types of loss functions. Some examples are:\n",
    "\n",
    "1. **Regression**: Mean Squared Error (MSE), Mean Absolute Error (MAE), Huber Loss  \n",
    "2. **Classification**: Binary Cross-Entropy, Categorical Cross-Entropy, Hinge Loss  \n",
    "3. **Autoencoders**: Kullbackâ€“Leibler (KL) Divergence  \n",
    "4. **GANs**: Discriminator Loss, Minâ€“Max GAN Loss  \n",
    "5. **Object Detection**: Focal Loss  \n",
    "6. **Embedding/Metric Learning**: Triplet Loss  \n",
    "\n",
    "ðŸ‘‰ Choosing the right loss function depends on the problem.  \n",
    "You can also design your **own custom loss function** if needed.\n",
    "\n",
    "\n",
    "### Loss vs. Cost Function\n",
    "- **Loss Function**: The error calculated for a **single data point**.  \n",
    "- **Cost Function**: The **average loss** across the entire dataset or a batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a98d5",
   "metadata": {},
   "source": [
    "### Some Common Loss Functions (with Advantages & Disadvantages)\n",
    "\n",
    "1. **Mean Squared Error (MSE)**  \n",
    "   - *Advantages*:  \n",
    "     - Easy to interpret  \n",
    "     - Differentiable  \n",
    "     - Has a single global minimum  \n",
    "   - *Disadvantages*:  \n",
    "     - Error units are squared (less intuitive)  \n",
    "     - Not robust to outliers  \n",
    "\n",
    "2. **Mean Absolute Error (MAE)**  \n",
    "   - *Advantages*:  \n",
    "     - Intuitive and easy to understand  \n",
    "     - Error units are the same as the target variable  \n",
    "     - Robust to outliers  \n",
    "   - *Disadvantages*:  \n",
    "     - Not differentiable at zero (can cause issues in optimization)  \n",
    "\n",
    "\n",
    "3. **Huber Loss**  \n",
    "   - A compromise between **MSE** and **MAE**:  \n",
    "     - For small errors (no outliers), behaves like **MSE**.  \n",
    "     - For large errors (outliers), behaves like **MAE**.  \n",
    "\n",
    "\n",
    "4. **Binary Cross-Entropy (Log Loss)**  \n",
    "   - Used for **binary classification problems** (e.g., *yes/no*).  \n",
    "   - *Advantages*:  \n",
    "     - Differentiable (works well with gradient descent)  \n",
    "   - *Disadvantages*:  \n",
    "     - Can have multiple local minima  \n",
    "     - Less intuitive to interpret directly  \n",
    "\n",
    "\n",
    "5. **Categorical Cross-Entropy**  \n",
    "   - Used for **multi-class classification** (with **softmax**).  \n",
    "   - Requires **one-hot encoding** of class labels.  \n",
    "\n",
    "\n",
    "6. **Sparse Categorical Cross-Entropy**  \n",
    "   - Similar to categorical cross-entropy, but:  \n",
    "     - Does **not** require one-hot encoding.  \n",
    "     - Input labels can be integers representing class indices.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
